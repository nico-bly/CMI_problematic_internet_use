{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/koko/Bureau/projets/kaggle/cmi_probleamtic_use/CMI_problematic_internet_use/datas\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get the current working directory\n",
    "current_path = Path.cwd()\n",
    "\n",
    "# Get the parent directory\n",
    "parent_path = current_path\n",
    "data_paths = parent_path / 'datas'  \n",
    "\n",
    "print(data_paths)\n",
    "#\n",
    "train_path = data_paths / 'train.csv'\n",
    "test_path = data_paths / 'test.csv'\n",
    "\n",
    "train_path_ts = data_paths / 'series_train.parquet'\n",
    "test_path_ts = data_paths / 'series_test.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from optuna import Trial, create_study\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Pre processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy\n",
    "\n",
    "We can make category per Age/sex\n",
    "\n",
    "for float take median "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_keep_test = ['id', 'Basic_Demos-Age', 'Basic_Demos-Sex','CGAS-CGAS_Score', 'Physical-BMI', 'Physical-Diastolic_BP','Physical-HeartRate','Physical-Systolic_BP','Fitness_Endurance-Max_Stage','Fitness_Endurance-Time_Sec', \n",
    "                    'FGC-FGC_CU_Zone','FGC-FGC_GSND_Zone','FGC-FGC_GSD_Zone', 'FGC-FGC_PU_Zone', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR_Zone','FGC-FGC_TL_Zone',\n",
    "                    'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI', 'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                    'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num','BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM', 'BIA-BIA_TBW',\n",
    "                    'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T', 'PreInt_EduHx-computerinternet_hoursday']\n",
    "\n",
    "features_to_keep_train = np.concatenate((features_to_keep_test,['sii']))\n",
    "                         \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from optuna import Trial, create_study\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(df, n_std=3):\n",
    "    \"\"\"\n",
    "    Detect outliers using z-score method\n",
    "    \"\"\"\n",
    "    outliers_mask = np.zeros(len(df), dtype=bool)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for column in numeric_cols:\n",
    "        if df[column].nunique() > 2:  # Skip binary columns\n",
    "            z_scores = np.abs(stats.zscore(df[column], nan_policy='omit'))\n",
    "            outliers_mask |= z_scores > n_std\n",
    "    return outliers_mask\n",
    "\n",
    "\n",
    "def preprocess_train(dataframe, features_to_keep, k=5, handle_outliers=True):\n",
    "    \"\"\"\n",
    "    Enhanced preprocessing function with advanced feature handling\n",
    "    \"\"\"\n",
    "    print(\"Starting preprocessing...\")\n",
    "    dataframe_selected = dataframe[features_to_keep].copy()\n",
    "    \n",
    "    # Drop columns with too many NaN values\n",
    "    columns_to_drop = []\n",
    "    for column in features_to_keep:\n",
    "        nan_ratio = dataframe[column].isna().sum() / len(dataframe[column])\n",
    "        if nan_ratio > 0.7:\n",
    "            columns_to_drop.append(column)\n",
    "    \n",
    "    dataframe_selected.drop(columns=columns_to_drop, inplace=True)\n",
    "    dataframe_selected = dataframe_selected.dropna(subset=[\"sii\"])\n",
    "    \n",
    "    # Store IDs and target\n",
    "    ids = dataframe_selected['id']\n",
    "    sii = dataframe_selected['sii']\n",
    "    \n",
    "    # Prepare data for imputation\n",
    "    train_data = dataframe_selected.drop(['id', 'sii'], axis=1)\n",
    "    \n",
    "    # Scale numeric features before imputation\n",
    "    scaler = StandardScaler()\n",
    "    numeric_cols = train_data.select_dtypes(include=[np.number]).columns\n",
    "    train_data[numeric_cols] = scaler.fit_transform(train_data[numeric_cols])\n",
    "    \n",
    "    # Enhanced KNN imputation\n",
    "    print(f\"Performing KNN imputation with k={k}...\")\n",
    "    model_imputer = KNNImputer(n_neighbors=k, weights='distance')\n",
    "    imputed_data = model_imputer.fit_transform(train_data)\n",
    "    \n",
    "    # Convert back to DataFrame\n",
    "    imputed_df = pd.DataFrame(imputed_data, columns=train_data.columns)\n",
    "    \n",
    "    # Handle outliers if requested\n",
    "    if handle_outliers:\n",
    "        print(\"Detecting and handling outliers...\")\n",
    "        outliers_mask = detect_outliers(imputed_df)\n",
    "        print(f\"Found {sum(outliers_mask)} outliers\")\n",
    "        \n",
    "        # Instead of removing outliers, clip them to acceptable ranges\n",
    "        for col in numeric_cols:\n",
    "            lower_bound = imputed_df[col].mean() - 3 * imputed_df[col].std()\n",
    "            upper_bound = imputed_df[col].mean() + 3 * imputed_df[col].std()\n",
    "            imputed_df[col] = imputed_df[col].clip(lower_bound, upper_bound)\n",
    "    \n",
    "    # Add back IDs and target\n",
    "    imputed_df['id'] = ids.values\n",
    "    imputed_df['sii'] = sii.values\n",
    "    \n",
    "    return imputed_df, columns_to_drop, model_imputer, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pipeline_train(dataframe_train, features_to_keep, k=3, optimize_hyperparams=True):\n",
    "    \"\"\"\n",
    "    Enhanced training pipeline with advanced preprocessing and model training\n",
    "    \"\"\"\n",
    "    # Preprocessing\n",
    "    imputed_df, columns_dropped, knn, scaler = preprocess_train(\n",
    "        dataframe_train, features_to_keep, k)\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = imputed_df.drop(['id', 'sii'], axis=1)\n",
    "    y = imputed_df['sii']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Train model\n",
    "    best_model, avg_accuracy, best_params = train_xgboost(\n",
    "        X_train, X_test, y_train, y_test, optimize_hyperparams)\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': avg_accuracy,\n",
    "        'model': best_model,\n",
    "        'columns_dropped': columns_dropped,\n",
    "        'knn_imputer': knn,\n",
    "        'scaler': scaler,\n",
    "        'feature_importance': feature_importance,\n",
    "        'best_params': best_params\n",
    "    }\n",
    "\n",
    "def find_best_k(train_data, features_to_keep, k_range=range(5, 33)):\n",
    "    \"\"\"\n",
    "    Find the optimal k value for KNN imputation\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for k in k_range:\n",
    "        print(f\"\\nTesting k={k}\")\n",
    "        pipeline_results = pipeline_train(\n",
    "            train_data, features_to_keep, k, optimize_hyperparams=False)\n",
    "        accuracy = pipeline_results['accuracy']\n",
    "        results.append({'k': k, 'accuracy': accuracy})\n",
    "        print(f'k={k}, Accuracy={accuracy * 100:.2f}%')\n",
    "    \n",
    "    best_result = max(results, key=lambda x: x['accuracy'])\n",
    "    print(f\"\\nBest k for imputation: {best_result['k']} \"\n",
    "          f\"with accuracy: {best_result['accuracy'] * 100:.2f}%\")\n",
    "    \n",
    "    return best_result['k']\n",
    "\n",
    "\n",
    "def train_xgboost(X_train, X_test, y_train, y_test, optimize_hyperparams=True):\n",
    "    \"\"\"\n",
    "    Enhanced XGBoost training with optional hyperparameter optimization\n",
    "    \"\"\"\n",
    "    if optimize_hyperparams:\n",
    "        print(\"Optimizing hyperparameters...\")\n",
    "        study = create_study(direction='maximize')\n",
    "        study.optimize(\n",
    "            lambda trial: objective(trial, X_train, y_train, num_class=4),\n",
    "            n_trials=50\n",
    "        )\n",
    "        best_params = study.best_params\n",
    "        best_params.update({\n",
    "            'objective': 'multi:softmax',\n",
    "            'num_class': 4,\n",
    "            'random_state': 42\n",
    "        })\n",
    "        print(\"Best parameters:\", best_params)\n",
    "    else:\n",
    "        best_params = {\n",
    "            'objective': 'multi:softmax',\n",
    "            'num_class': 4,\n",
    "            'learning_rate': 0.1,\n",
    "            'max_depth': 6,\n",
    "            'n_estimators': 100,\n",
    "            'random_state': 42,\n",
    "            'alpha': 0.0\n",
    "        }\n",
    "    \n",
    "    # Use StratifiedKFold for better handling of imbalanced classes\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracy_scores = []\n",
    "    models = []\n",
    "    \n",
    "    for train_index, val_index in skf.split(X_train, y_train):\n",
    "        X_train_fold = X_train.iloc[train_index]\n",
    "        X_val_fold = X_train.iloc[val_index]\n",
    "        y_train_fold = y_train.iloc[train_index]\n",
    "        y_val_fold = y_train.iloc[val_index]\n",
    "        \n",
    "        model = xgb.XGBClassifier(**best_params)\n",
    "        \n",
    "        # Train the model with eval_set but without early_stopping_rounds\n",
    "        model.fit(\n",
    "            X_train_fold, \n",
    "            y_train_fold,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        y_pred = model.predict(X_val_fold)\n",
    "        accuracy = accuracy_score(y_val_fold, y_pred)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        models.append(model)\n",
    "    \n",
    "    best_index = np.argmax(accuracy_scores)\n",
    "    best_model = models[best_index]\n",
    "    avg_accuracy = np.mean(accuracy_scores)\n",
    "    \n",
    "    return best_model, avg_accuracy, best_params\n",
    "\n",
    "def objective(trial: Trial, X_train, y_train, num_class):\n",
    "    \"\"\"\n",
    "    Optuna objective function for XGBoost hyperparameter optimization\n",
    "    \"\"\"\n",
    "    param = {\n",
    "        'objective': 'multi:softmax',\n",
    "        'num_class': num_class,\n",
    "        'tree_method': 'hist',\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "    }\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, valid_idx in cv.split(X_train, y_train):\n",
    "        X_train_fold = X_train.iloc[train_idx]\n",
    "        y_train_fold = y_train.iloc[train_idx]\n",
    "        X_valid_fold = X_train.iloc[valid_idx]\n",
    "        y_valid_fold = y_train.iloc[valid_idx]\n",
    "        \n",
    "        model = xgb.XGBClassifier(**param, random_state=42)\n",
    "        model.fit(\n",
    "            X_train_fold, \n",
    "            y_train_fold,\n",
    "            eval_set=[(X_valid_fold, y_valid_fold)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        pred = model.predict(X_valid_fold)\n",
    "        fold_score = accuracy_score(y_valid_fold, pred)\n",
    "        scores.append(fold_score)\n",
    "    \n",
    "    return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test(test_data, features_to_keep, columns_to_drop, knn_model, scaler):\n",
    "    \"\"\"\n",
    "    Preprocess test data consistently with training data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    test_data : pandas DataFrame\n",
    "        The test dataset\n",
    "    features_to_keep : list\n",
    "        List of features to keep in the preprocessing\n",
    "    columns_to_drop : list\n",
    "        Columns that were dropped during training\n",
    "    knn_model : KNNImputer\n",
    "        Fitted KNN imputer from training\n",
    "    scaler : StandardScaler\n",
    "        Fitted scaler from training\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing test data...\")\n",
    "    \n",
    "    # Select and drop columns consistently with training\n",
    "    dataframe_selected = test_data[features_to_keep].copy()\n",
    "    if columns_to_drop:\n",
    "        dataframe_selected.drop(columns=columns_to_drop, inplace=True)\n",
    "    \n",
    "    # Store IDs\n",
    "    ids = dataframe_selected['id']\n",
    "    \n",
    "    # Prepare data for imputation\n",
    "    test_features = dataframe_selected.drop(['id'], axis=1)\n",
    "    \n",
    "    # Scale numeric features consistently with training\n",
    "    numeric_cols = test_features.select_dtypes(include=[np.number]).columns\n",
    "    test_features[numeric_cols] = scaler.transform(test_features[numeric_cols])\n",
    "    \n",
    "    # Apply KNN imputation\n",
    "    print(\"Applying KNN imputation...\")\n",
    "    imputed_data = knn_model.transform(test_features)\n",
    "    \n",
    "    # Convert back to DataFrame\n",
    "    imputed_df = pd.DataFrame(imputed_data, columns=test_features.columns)\n",
    "    imputed_df['id'] = ids.values\n",
    "    \n",
    "    return imputed_df\n",
    "\n",
    "def predict_test_data(test_data, model, pipeline_results, features_to_keep_test):\n",
    "    \"\"\"\n",
    "    Complete pipeline for processing test data and making predictions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    test_data : pandas DataFrame\n",
    "        The test dataset\n",
    "    model : XGBClassifier\n",
    "        Trained model\n",
    "    pipeline_results : dict\n",
    "        Results from training pipeline including preprocessors\n",
    "    features_to_keep_test : list\n",
    "        Features to use for test data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (predictions, processed_test_data)\n",
    "        Predictions and the processed test data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract preprocessors from pipeline results\n",
    "        knn_imputer = pipeline_results['knn_imputer']\n",
    "        scaler = pipeline_results['scaler']\n",
    "        columns_dropped = pipeline_results['columns_dropped']\n",
    "        \n",
    "        # Preprocess test data\n",
    "        processed_test = preprocess_test(\n",
    "            test_data,\n",
    "            features_to_keep_test,\n",
    "            columns_dropped,\n",
    "            knn_imputer,\n",
    "            scaler\n",
    "        )\n",
    "        \n",
    "        # Prepare features for prediction\n",
    "        test_features = processed_test.drop('id', axis=1)\n",
    "        \n",
    "        # Ensure all columns are numeric\n",
    "        numeric_test = test_features.select_dtypes(include=[np.number])\n",
    "        \n",
    "        # Check for any remaining non-numeric columns\n",
    "        non_numeric_cols = set(test_features.columns) - set(numeric_test.columns)\n",
    "        if non_numeric_cols:\n",
    "            print(f\"Warning: Found non-numeric columns: {non_numeric_cols}\")\n",
    "            print(\"Converting to numeric...\")\n",
    "            test_features = test_features.apply(pd.to_numeric, errors='coerce')\n",
    "        \n",
    "        # Make predictions\n",
    "        print(\"Making predictions...\")\n",
    "        predictions = model.predict(test_features)\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        results_df = pd.DataFrame({\n",
    "            'id': processed_test['id'],\n",
    "            'sii': predictions\n",
    "        })\n",
    "        \n",
    "        print(\"Prediction complete!\")\n",
    "        return results_df, processed_test\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in prediction pipeline: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_pipeline(train_data, test_data, features_to_keep_train, features_to_keep_test, k=19):\n",
    "    \"\"\"\n",
    "    Run complete pipeline from training through prediction\n",
    "    \"\"\"\n",
    "    # Train the model\n",
    "    print(\"Training model...\")\n",
    "    pipeline_results = pipeline_train(train_data, features_to_keep_train, k=k, optimize_hyperparams=True)\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"Processing test data and making predictions...\")\n",
    "    predictions_df, processed_test = predict_test_data(\n",
    "        test_data,\n",
    "        pipeline_results['model'],\n",
    "        pipeline_results,\n",
    "        features_to_keep_test\n",
    "    )\n",
    "    \n",
    "    return predictions_df, pipeline_results, processed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Starting preprocessing...\n",
      "Performing KNN imputation with k=20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 17:50:23,264] A new study created in memory with name: no-name-0b848c85-b70f-4000-a541-e1fd4603cac7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting and handling outliers...\n",
      "Found 210 outliers\n",
      "Optimizing hyperparameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 17:50:25,125] Trial 0 finished with value: 0.5950607608956877 and parameters: {'learning_rate': 0.2964191484933706, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.890469399504168, 'colsample_bytree': 0.740481618479278, 'gamma': 1.8203736329394578, 'alpha': 0.2037532099688386, 'n_estimators': 153}. Best is trial 0 with value: 0.5950607608956877.\n",
      "[I 2024-12-08 17:50:28,982] Trial 1 finished with value: 0.5850119640972593 and parameters: {'learning_rate': 0.11841886701706443, 'max_depth': 4, 'min_child_weight': 4, 'subsample': 0.9495389064524977, 'colsample_bytree': 0.7902224229634198, 'gamma': 0.4543195800451971, 'alpha': 0.01002029151676504, 'n_estimators': 239}. Best is trial 0 with value: 0.5950607608956877.\n",
      "[I 2024-12-08 17:50:31,172] Trial 2 finished with value: 0.6037532783716288 and parameters: {'learning_rate': 0.07155958652369467, 'max_depth': 12, 'min_child_weight': 7, 'subsample': 0.8994887001969856, 'colsample_bytree': 0.9677639825641907, 'gamma': 2.8008072287866654, 'alpha': 0.02318686139166329, 'n_estimators': 134}. Best is trial 2 with value: 0.6037532783716288.\n",
      "[I 2024-12-08 17:50:34,733] Trial 3 finished with value: 0.5955341002894372 and parameters: {'learning_rate': 0.1935121527098651, 'max_depth': 4, 'min_child_weight': 4, 'subsample': 0.9241301475992098, 'colsample_bytree': 0.810242402807318, 'gamma': 1.413187211193896, 'alpha': 0.03467094178785071, 'n_estimators': 230}. Best is trial 2 with value: 0.6037532783716288.\n",
      "[I 2024-12-08 17:50:36,703] Trial 4 finished with value: 0.6078639123120487 and parameters: {'learning_rate': 0.130920031505685, 'max_depth': 3, 'min_child_weight': 4, 'subsample': 0.6223793849141749, 'colsample_bytree': 0.6769553876301793, 'gamma': 2.7180511366956024, 'alpha': 1.0419562721012794, 'n_estimators': 206}. Best is trial 4 with value: 0.6078639123120487.\n",
      "[I 2024-12-08 17:50:39,707] Trial 5 finished with value: 0.6023750561633386 and parameters: {'learning_rate': 0.16602409642067756, 'max_depth': 3, 'min_child_weight': 7, 'subsample': 0.851054458968868, 'colsample_bytree': 0.6997727351701623, 'gamma': 1.9925977025843404, 'alpha': 2.8736241231849418, 'n_estimators': 294}. Best is trial 4 with value: 0.6078639123120487.\n",
      "[I 2024-12-08 17:50:41,231] Trial 6 finished with value: 0.5955257410948456 and parameters: {'learning_rate': 0.21545483555255326, 'max_depth': 7, 'min_child_weight': 3, 'subsample': 0.9041092472132458, 'colsample_bytree': 0.7020043598077669, 'gamma': 1.9090173473587957, 'alpha': 0.0088877961084234, 'n_estimators': 132}. Best is trial 4 with value: 0.6078639123120487.\n",
      "[I 2024-12-08 17:50:44,173] Trial 7 finished with value: 0.6055849868865135 and parameters: {'learning_rate': 0.048547509522128114, 'max_depth': 8, 'min_child_weight': 5, 'subsample': 0.6708907254176624, 'colsample_bytree': 0.8616290871778743, 'gamma': 3.4800485553539984, 'alpha': 0.08004529812046322, 'n_estimators': 279}. Best is trial 4 with value: 0.6078639123120487.\n",
      "[I 2024-12-08 17:50:45,811] Trial 8 finished with value: 0.5891267776349748 and parameters: {'learning_rate': 0.08438255945450372, 'max_depth': 5, 'min_child_weight': 6, 'subsample': 0.7893919274630756, 'colsample_bytree': 0.695051052557808, 'gamma': 1.1083725468122503, 'alpha': 0.0650797286249652, 'n_estimators': 71}. Best is trial 4 with value: 0.6078639123120487.\n",
      "[I 2024-12-08 17:50:48,008] Trial 9 finished with value: 0.6064930043990262 and parameters: {'learning_rate': 0.10827411546108008, 'max_depth': 8, 'min_child_weight': 3, 'subsample': 0.9214670742484541, 'colsample_bytree': 0.6400265808925729, 'gamma': 2.941851824890869, 'alpha': 0.002299567382143457, 'n_estimators': 259}. Best is trial 4 with value: 0.6078639123120487.\n",
      "[I 2024-12-08 17:50:49,592] Trial 10 finished with value: 0.5959802723007639 and parameters: {'learning_rate': 0.25558131698670195, 'max_depth': 10, 'min_child_weight': 1, 'subsample': 0.6159985053209465, 'colsample_bytree': 0.6003212898254263, 'gamma': 4.7984510239202525, 'alpha': 8.163939190888893, 'n_estimators': 200}. Best is trial 4 with value: 0.6078639123120487.\n",
      "[I 2024-12-08 17:50:51,826] Trial 11 finished with value: 0.6069527601015642 and parameters: {'learning_rate': 0.12961507257260657, 'max_depth': 8, 'min_child_weight': 3, 'subsample': 0.753504415127553, 'colsample_bytree': 0.6092900879172, 'gamma': 3.2098623350981352, 'alpha': 0.0011384402850645073, 'n_estimators': 241}. Best is trial 4 with value: 0.6078639123120487.\n",
      "[I 2024-12-08 17:50:56,086] Trial 12 finished with value: 0.5996311505386457 and parameters: {'learning_rate': 0.021271189201797464, 'max_depth': 7, 'min_child_weight': 3, 'subsample': 0.743261204879289, 'colsample_bytree': 0.6001979460704624, 'gamma': 3.909900716236254, 'alpha': 0.6891994057009029, 'n_estimators': 197}. Best is trial 4 with value: 0.6078639123120487.\n",
      "[I 2024-12-08 17:50:58,707] Trial 13 finished with value: 0.6019226147560682 and parameters: {'learning_rate': 0.13567242533212454, 'max_depth': 10, 'min_child_weight': 2, 'subsample': 0.6852951702889616, 'colsample_bytree': 0.652052462739846, 'gamma': 4.155867670911091, 'alpha': 0.37280258665138793, 'n_estimators': 208}. Best is trial 4 with value: 0.6078639123120487.\n",
      "[I 2024-12-08 17:51:01,077] Trial 14 finished with value: 0.6019173902594485 and parameters: {'learning_rate': 0.17111659033220505, 'max_depth': 10, 'min_child_weight': 5, 'subsample': 0.7324966609234141, 'colsample_bytree': 0.7703017785140096, 'gamma': 3.157748520772759, 'alpha': 0.0011558770760510893, 'n_estimators': 170}. Best is trial 4 with value: 0.6078639123120487.\n",
      "[I 2024-12-08 17:51:02,673] Trial 15 finished with value: 0.6042057197788994 and parameters: {'learning_rate': 0.22283274734217812, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.6093467823806037, 'colsample_bytree': 0.9041587737208159, 'gamma': 2.440558450190116, 'alpha': 1.7319149771867846, 'n_estimators': 99}. Best is trial 4 with value: 0.6078639123120487.\n",
      "[I 2024-12-08 17:51:05,376] Trial 16 finished with value: 0.6046623407834655 and parameters: {'learning_rate': 0.14425217408491767, 'max_depth': 9, 'min_child_weight': 5, 'subsample': 0.9959058314376806, 'colsample_bytree': 0.6544607336538717, 'gamma': 3.856600673408155, 'alpha': 1.0018936966411396, 'n_estimators': 246}. Best is trial 4 with value: 0.6078639123120487.\n",
      "[I 2024-12-08 17:51:07,789] Trial 17 finished with value: 0.5946020500924736 and parameters: {'learning_rate': 0.09187737400792234, 'max_depth': 3, 'min_child_weight': 2, 'subsample': 0.8055531028866231, 'colsample_bytree': 0.7314988350062179, 'gamma': 4.806709838895209, 'alpha': 6.795126192035545, 'n_estimators': 217}. Best is trial 4 with value: 0.6078639123120487.\n",
      "[I 2024-12-08 17:51:10,893] Trial 18 finished with value: 0.6010135523442316 and parameters: {'learning_rate': 0.13074175543779348, 'max_depth': 12, 'min_child_weight': 4, 'subsample': 0.6646573977868023, 'colsample_bytree': 0.6428871246070232, 'gamma': 2.4603121566826895, 'alpha': 0.003339059348738214, 'n_estimators': 263}. Best is trial 4 with value: 0.6078639123120487.\n"
     ]
    }
   ],
   "source": [
    "# Run the complete pipeline\n",
    "predictions_df, pipeline_results, processed_test = run_complete_pipeline(\n",
    "    train_data=train,\n",
    "    test_data=test,\n",
    "    features_to_keep_train=features_to_keep_train,\n",
    "    features_to_keep_test=features_to_keep_test,\n",
    "    k=20 \n",
    ")\n",
    "\n",
    "# Access results\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"Training Accuracy: {pipeline_results['accuracy']*100:.2f}%\")\n",
    "print(\"\\nPrediction Summary:\")\n",
    "print(predictions_df['sii'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_cmi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
